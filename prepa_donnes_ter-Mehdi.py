# -*- coding: utf-8 -*-
"""prepa-donnes-TER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ssZOCBB7u3jkEwcn4hrz5hzhBPydQabi
"""

from google.colab import drive
drive.mount('/content/drive')
### données trop lourdes donc préparation des données via un lien google drive

from google.colab import drive
import pandas as pd

# Montez Google Drive dans Colab
drive.mount('/content/drive')

chemin_fichier = '/content/drive/My Drive/resultat_cancer.txt'
chunk_size = 100000  # Ajustez cette taille si nécessaire

# Initialiser un DataFrame vide pour le résultat final
df_pivot_final = pd.DataFrame()

chunks = pd.read_csv(chemin_fichier, delimiter='\t', chunksize=chunk_size)

for chunk in chunks:
    # Aggrégez les données pour les doublons
    aggregated = chunk.groupby(['sequence_name', 'motif_alt_id'])['p-value'].mean().reset_index()

    pivot_chunk = aggregated.pivot(index='sequence_name', columns='motif_alt_id', values='p-value')

    # Fusionnez le morceau pivoté avec le DataFrame global
    df_pivot_final = df_pivot_final.combine_first(pivot_chunk)

# Affichez les premières lignes du résultat
df_pivot_final.head()

df_pivot_final.to_csv('resultat_pivote.csv', index=True)


# Télécharger le fichier CSV
from google.colab import files
files.download('resultat_pivote_final1.csv')

# Supposons que df est votre DataFrame après avoir lu le fichier
df = df.set_index(['sequence_name', 'motif_id', 'motif_alt_id'])['p-value'].unstack(level=[1,2])

from google.colab import drive
import pandas as pd

# Montez Google Drive dans Colab
drive.mount('/content/drive')

chemin_fichier = '/content/drive/My Drive/resultat_cancer.txt'
chunk_size = 100000  # Ajustez cette taille si nécessaire

# Initialiser un DataFrame vide pour le résultat final
df_pivot_final = pd.DataFrame()

chunks = pd.read_csv(chemin_fichier, delimiter='\t', chunksize=chunk_size)

for chunk in chunks:
    # Aggrégez les données pour les doublons
    aggregated = chunk.groupby(['sequence_name', 'motif_id', 'motif_alt_id'])['p-value'].mean().reset_index()

    # Pivotez avec multi-index pour les colonnes
    pivot_chunk = aggregated.pivot_table(index='sequence_name', columns=['motif_id', 'motif_alt_id'], values='p-value')

    # Fusionnez le morceau pivoté avec le DataFrame global
    df_pivot_final = df_pivot_final.combine_first(pivot_chunk)

# Affichez les premières lignes du résultat
df_pivot_final.head()

## debut création matrice avec les sequence en premiere colonne et les nom de genere en ligne et p-valeur associé ()
#pivot de la matrice
from google.colab import drive
import pandas as pd

# Montez Google Drive dans Colab
drive.mount('/content/drive')

chemin_fichier = '/content/drive/My Drive/resultat_cancer.txt'
chunk_size = 100000  # Ajustez cette taille si nécessaire

# Initialiser un DataFrame vide pour le résultat final
df_pivot_final = pd.DataFrame()

chunks = pd.read_csv(chemin_fichier, delimiter='\t', chunksize=chunk_size)

for chunk in chunks:

    aggregated = chunk.groupby(['sequence_name', 'motif_alt_id', 'motif_id'])['p-value'].mean().reset_index()

    # Création d'une colonne combinant motif_id et motif_alt_id pour servir de colonne pivot
    aggregated['motif_combined'] = aggregated['motif_id'] + "::" + aggregated['motif_alt_id']

    pivot_chunk = aggregated.pivot(index='sequence_name', columns='motif_combined', values='p-value')

    # Fusion le morceau pivoté avec le DataFrame global
    df_pivot_final = df_pivot_final.combine_first(pivot_chunk)

# les premières lignes du résultat
df_pivot_final.head()

# Affichez les premières lignes du résultat
df_pivot_final.head()

# Télécharger le fichier CSV
from google.colab import files
files.download('resultat_pivote_final1.csv')

df_pivot_final.to_csv('resultat_pivote_final1.csv', index=True)

##### code pour avoir la plus petite pvaleur et avoir la matrice

from google.colab import drive
import pandas as pd

# Montez Google Drive dans Colab
drive.mount('/content/drive')

chemin_fichier = '/content/drive/My Drive/resultat_cancer.txt'
chunk_size = 100000  # Ajustez cette taille si nécessaire

# Initialiser un DataFrame vide pour les données agrégées
df_aggregated_final = pd.DataFrame()

# Lire le fichier texte par chunk
chunks = pd.read_csv(chemin_fichier, delimiter='\t', chunksize=chunk_size)

for chunk in chunks:
    # Aggrégez les données pour les doublons en prenant la plus petite p-valeur
    aggregated = chunk.groupby(['sequence_name', 'motif_alt_id', 'motif_id'])['p-value'].min().reset_index()

    # Si df_aggregated_final est vide, copiez simplement aggregated
    if df_aggregated_final.empty:
        df_aggregated_final = aggregated
    else:
        # Sinon, fusionnez les données, en gardant la p-valeur minimale
        df_aggregated_final = pd.concat([df_aggregated_final, aggregated])
        df_aggregated_final = df_aggregated_final.groupby(['sequence_name', 'motif_alt_id', 'motif_id'])['p-value'].min().reset_index()

# Maintenant que nous avons toutes les p-valeurs minimales, nous pouvons créer la matrice pivotée
df_pivot_final3 = df_aggregated_final.pivot(index='sequence_name', columns='motif_id', values='p-value')

# Remplacer les NaN par des zéros ou une autre valeur si nécessaire
df_pivot_final3.fillna(0, inplace=True)

# Afficher les premières lignes du résultat final
df_pivot_final3.head()

!pip install pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").appName("Load_CSV").getOrCreate()

from google.colab import drive
drive.mount('/content/drive')

# Remplacez 'path_to_file' par le chemin d'accès à votre fichier CSV
df = spark.read.csv('/content/drive/My Drive/resultat_pivote_final1.csv', header=True, inferSchema=True)

df.show()

# On souhaite faire la joiture avec le fichier txt
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as sql_sum

#session Spark
spark = SparkSession.builder.master("local[*]").appName("Data_Join").getOrCreate()

#data frame précédent déja charger

# Charger le fichier texte

df_txt = spark.read.csv('/content/drive/My Drive/exprs_data.txt', sep='\t', header=True, inferSchema=True)



df_joined = df.join(df_txt, df.sequence_name == df_txt.gene_id)
#erreur joiture avec s

# Afficher le résultat pour vérification
df_joined.show()

### On a des valeurs nulles donc on doit les traiter

# Convertir le DataFrame en RDD (car les colonnes posent probleme  avec leur nom complexe ==> passage obligatoire en python )
rdd = df_joined.rdd


def count_nulls(row):
    return sum([1 for value in row if value is None])

total_null_count = rdd.map(count_nulls).sum()

print("Nombre total de valeurs NULL dans le DataFrame : ", total_null_count)

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Démarrer la session Spark
spark = SparkSession.builder.master("local[*]").appName("Data_Analysis").getOrCreate()


# df_joined = ...

# Remplacer les NaN ou NULL par 2 * 10^-3 dans tout le DataFrame
value_to_replace = 2 * (10 ** -3)
df_joined_filled = df_joined.na.fill(value_to_replace)

# Préparer les expressions pour la transformation -log(10) pour chaque colonne sauf 'sequence_name'
transform_exprs = ["sequence_name"]  # Conserver 'sequence_name' inchangée
transform_exprs += [
    f"CASE WHEN `{col}` IS NOT NULL THEN -LOG10(ABS(`{col}`)) ELSE `{col}` END as `{col}`"
    for col in df_joined.columns if col != "sequence_name"
]

# Appliquer les transformations en utilisant selectExpr
df_transformed = df_joined_filled.selectExpr(*transform_exprs)


## on a remplacer les valeurs null par (2 * 10^-3) des NULL qui restaient et mettre en -log(10)
from pyspark.sql.functions import when

# Valeur à utiliser pour remplacer les NULL
value_to_replace = 2 * (10 ** -3)

# Préparer les expressions pour remplacer les NULL par 2 * 10^-3 dans toutes les colonnes
# On crée une nouvelle colonne pour chaque colonne existante,
# en remplaçant les valeurs NULL par 2 * 10^-3
exprs_all_cols = [
    f"CASE WHEN `{col}` IS NULL THEN {value_to_replace} ELSE `{col}` END AS `{col}`"
    for col in df_transformed.columns
]

# Appliquer les transformations au DataFrame
# La méthode selectExpr permet d'appliquer les expressions définies ci-dessus
df_transformed_all_filled = df_transformed.selectExpr(*exprs_all_cols)

# Afficher les premières lignes du DataFrame pour vérification
df_transformed_all_filled.show()

# Fonction pour compter les valeurs NULL dans une ligne
def count_nulls_rdd(row):
    return sum(value is None for value in row)

# Compter le nombre total de valeurs NULL dans le DataFrame après le traitement
# On utilise une transformation RDD pour parcourir chaque ligne
total_null_count_rdd_all = df_transformed_all_filled.rdd.map(count_nulls_rdd).sum()

# Afficher le nombre total de valeurs NULL
print("Nombre total de valeurs NULL dans le DataFrame après traitement étendu : ", total_null_count_rdd_all)



### Peut on filtrer sur du 10^-2 a partir de la matrice join ?

# Analyse descritptive des NAN ou NULL, remplacer par 2*(10^-3) = ok
#Passer la matrice à -log(10) de la p valeur ----> construction score = ok
#repartir en classe Y avec seuil 2 et 1/2 , matrice X et Y , seuil proposé 2 et 1/2????
# analyse descriptive R
#Lien entre les variables ( regarder a l interieur de la matrice X) , corrélation 2 a 2 , conditionnement de X^TX , regression ineaire , regression logistique ( important)
#ACP et clustering ( premiere approche classification hierarchique) , matrice des distances 2 a 2 ( correlation), ( fusionner corr....etc) , avec hclust ,,-->
# --> choisir un seuil de coupure.avec MLGL c le lasso qui choisis le coupure pour chaque groupe de variable
# Entre les colonnes de X et chaque colonne de Y ( regression logistique)
# regrader algotithme ( forward)
# regarder la regression penalisée ( max de vraisemblance penaliser , minimiser la valeur des coeeff ) = LASSO = penaité en l(1)
# regrader les contraintes en normes l1 et l2
#elasticnet???
# utiliser le mpackage glmnet pour lasso ( decouvrir) et idelalement tester sur les données.
#PREVOIR desc ritères d'évaluations du modele :  précision rappel ( courbes rocke , matrice de confusion)

## a plus long terme
#un fichier par tache en RMARKDOWN
#preeiction classse up vs null , down vs null , ( up+down) vs null

# Définir le chemin du fichier de sortie sur le bureau
output_path = "/Users/lise/Desktop/df_transformed1.csv"

# Enregistrer le DataFrame en tant que fichier CSV
# Notez que Spark enregistre chaque partition du DataFrame en tant que fichier séparé dans un dossier
df_transformed.coalesce(1).write.csv(output_path, header=True, mode="overwrite")

# Enregistrer le DataFrame en tant que fichier CSV dans Colab
output_path_colab = "/content/df_transformed"
df_transformed.coalesce(1).write.option("header", "true").csv(output_path_colab, mode="overwrite")

# Combiner les fichiers CSV en un seul (car Spark génère plusieurs fichiers)
import os
import shutil
output_single_file = "/content/df_transformed1.csv"

# Trouver le fichier CSV partiel et le renommer
part_file = next(
    (f for f in os.listdir(output_path_colab) if f.startswith("part-")), None
)
if part_file:
    shutil.move(os.path.join(output_path_colab, part_file), output_single_file)

# Télécharger le fichier CSV combiné
from google.colab import files
files.download(output_single_file)

from pyspark.sql.functions import count, col

# Compter le nombre d'occurrences de chaque valeur dans 'sequence_name'
df_sequence_counts = df_transformed.groupBy("sequence_name").agg(count("*").alias("count"))

# Filtrer pour trouver les valeurs qui apparaissent plus d'une fois
df_duplicates = df_sequence_counts.filter(col("count") > 1)

# Afficher les doublons
df_duplicates.show()

# Compter le nombre de doublons
num_duplicates = df_duplicates.count()
print(f"Nombre de doublons dans 'sequence_name': {num_duplicates}")

df_transformed.show()

# Extraction des données nécessaires pour l'histogramme
sequence_counts = df_sequence_counts.toPandas()
# Nombre de top séquences à afficher
top_n = 10

# Extraction des données pour le graphique des top N séquences
top_sequences = df_sequence_counts.orderBy(col("count").desc()).limit(top_n).toPandas()
# Extraction des données pour la répartition des doublons
duplicates_distribution = df_duplicates.toPandas()

df_pandas = df_sequence_counts.toPandas()
print(df_pandas.head())

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assumons que df_transformed est un DataFrame Pandas avec des données numériques
# Vous pouvez le convertir de Spark à Pandas si nécessaire

